#!/usr/bin/python3
# -*- coding: utf-8 -*-

import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import tldextract
import argparse
from quo import echo


DEPTH = 5

def config_download_path(download_path):
    """
        create directory if not exist download_path
    """
    if not os.path.exists(download_path):
        os.makedirs(download_path)

def is_valid_image_url(url):
    """
    Checks if the provided URL is valid for an image.
    This function modifies the URL's scheme to 'http', and then checks if the URL starts with 'http'. 
    This ensures that the URL is properly formatted for an image on the web.

    Args:
        url (str): The URL of the image to check.

    Returns:
        bool: True if the URL is valid for an image, False otherwise.
    """
    url_with_http_scheme = urlparse(url)._replace(scheme='http').geturl()
    return url_with_http_scheme.startswith('http')

def download_image(url, path):
    """
    Downloads an image from a given URL and saves it to the specified path.

    This function attempts to download an image from the provided URL. If the download is successful 
    (HTTP status code 200), the image is saved to the given path. If the download fails, it prints 
    an error message. The function also prints a progress indicator (✅ for success, ❌ for failure).

    Args:
        url (str): The URL of the image to download.
        path (str): The file system path where the image should be saved.

    Returns:
        None
    """
    print(f"\t[{url[:50]}] ... ", end='')
    try:
        response = requests.get(url)
        if response.status_code == 200:
            with open(path, 'wb') as file:
                    file.write(response.content)
            print("✅")
        else:
            print("❌")
    except Exception as e:
        print(f"❌ : {e}")

def extract_images_from_page(url, path, downloaded_img):
    """
    Extracts and downloads images from a given webpage URL.

    This function fetches the contents of the webpage at the specified URL. It then parses the HTML 
    content to find all image tags (<img> and <picture>). For each image, it constructs the full image URL 
    and downloads the image to the specified path, avoiding re-downloading images already downloaded.

    Args:
        url (str): The URL of the webpage from which to extract images.
        path (str): The local file system path where the images should be saved.
        downloaded_img (set): A set to keep track of images already downloaded.

    Returns:
        None
    """
    echo(f"Extract from {url} : ",underline=True, fg="blue",nl = False)
    try:
        response = requests.get(url)
        if response.status_code == 200:
            print()
            soup = BeautifulSoup(response.text, 'html.parser')
            img_tags = soup.find_all('img') + soup.find_all('picture')
            for img_tag in img_tags:
                if img_tag.name == 'picture':
                    # If the tag is a <picture>, extract the source from the <source> tag
                    source_tag = img_tag.find('source')
                    if source_tag:
                        img_url = source_tag.get('srcset', '').split()[0]
                    else:
                        # If no <source> tag, use the <img> tag
                        img_url = img_tag.get('src')
                else:
                    img_url = img_tag.get('src')
                
                if img_url and is_valid_image_url(img_url):
                    img_url = urljoin(url, img_url)
                    img_url = urlparse(img_url)._replace(scheme='http').geturl()
                    img_name = os.path.basename(urlparse(img_url).path)
                    img_path = os.path.join(path, img_name)
                    if img_url not in downloaded_img:
                        download_image(img_url, img_path)
                        downloaded_img.add(img_url)
        else:
            print(f"Bad response -> {response}")
    except Exception as e:
        print(f"Error in request : {e}")

def spider(url, recursion_depth, download_path, base_domain, visited_urls, downloaded_img):
    """
    Recursively crawls a website from the given URL to a specified depth, downloading images.

    This function visits a URL, downloads images from that page, and follows the links on the page to 
    other pages within the same domain, up to the specified recursion depth. It avoids visiting the same 
    URL more than once and only downloads images from the base domain.

    Args:
        url (str): The starting URL for the spider to crawl.
        recursion_depth (int): The maximum depth of recursion for visiting links.
        download_path (str): The directory path where images will be downloaded.
        base_domain (str): The base domain from which images should be downloaded.
        visited_urls (set): A set to keep track of URLs already visited.
        downloaded_img (set): A set to keep track of images already downloaded.

    Returns:
        None
    """
    if recursion_depth == 0 or url in visited_urls:
        return
    visited_urls.add(urljoin(base_domain, url))
    if not os.path.exists(download_path):
        os.makedirs(download_path)

    link_domain = tldextract.extract(url).domain
    # print(f"link_domain = {link_domain}")
    if link_domain == base_domain:
        extract_images_from_page(url, download_path, downloaded_img)
    
    # Recursively follow links and download images
    try:
        responses = requests.get(url)
        if responses.status_code == 200:
            soup = BeautifulSoup(responses.text, 'html.parser')
            links = soup.find_all('a')
            for link in links:
                href = link.get('href')
                # print(f"{DEPTH - recursion_depth} - href = {href}")
                tabs = '\t' * (DEPTH - recursion_depth + 1)
                print(f"\n{tabs}|\n{tabs}└> [{href}]", end= '')
                if href:
                    link_url = urljoin(url, href)
    
                    # Ajouter le schéma 'http' si l'URL relative ne contient pas de schéma
                    if urlparse(link_url).scheme == '':
                        link_url = urlparse(link_url)._replace(scheme='http').geturl()

                    link_domain = tldextract.extract(link_url).domain
                    if link_domain == base_domain:
                        spider(link_url, recursion_depth - 1, download_path, base_domain, visited_urls, downloaded_img)
                    else:
                        print()
                else:
                    print()
        else:
            print(f"Bad response -> {responses}")
    except Exception as e:
        print(f"Error in request : {e}")
        
if __name__ == "__main__":
    downloaded_img = set()
    visited_urls = set()
    parser = argparse.ArgumentParser(description="Spider program to extract images from a website.")
    parser.add_argument("url", help="URL of the website")
    parser.add_argument("-r", action="store_true", help="Recursively download images")
    parser.add_argument("-l", type=int, default=DEPTH, help="Maximum depth level for recursive download")
    parser.add_argument("-p", type=str, default="./data/", help="Path to save downloaded files")

    args = parser.parse_args()
    base_domain = tldextract.extract(args.url).domain
    DEPTH = args.l
    print(f"base_domaine = {base_domain}")
    config_download_path(args.p)
    if args.r:
        spider(args.url, args.l, args.p, base_domain, visited_urls, downloaded_img)
    else:
        extract_images_from_page(args.url, args.p, downloaded_img)
    print(f"downloaded images #= {len(downloaded_img)}")
    if len(visited_urls):
        print(f"visited links #= {len(visited_urls)}")